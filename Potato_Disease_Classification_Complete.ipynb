{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## üìö Data Collection Methods\n",
        "\n",
        "Any data science project starts with collecting data. There are **three main ways** to collect data:\n",
        "\n",
        "### 1. Buy data from third-party vendors\n",
        "- Purchase pre-collected and labeled datasets\n",
        "- Good for specific use cases when data is available commercially\n",
        "\n",
        "### 2. Collect and annotate data on your own\n",
        "- Manually collect images/data from field\n",
        "- Work with domain experts (e.g., farmers, doctors) to label the data\n",
        "- Time-consuming but gives you custom dataset\n",
        "\n",
        "### 3. Use publicly available datasets\n",
        "- Download free datasets from platforms like Kaggle, Google, GitHub\n",
        "- Suitable for learning and research projects\n",
        "- **This project uses PlantVillage dataset** (publicly available)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "7gYkpp4PKoCL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FEe40qRF-Td",
        "outputId": "14b61706-6f01-445b-bfb2-0ed62bf6e8e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.19.0\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Potato Disease Classification - Data Collection & Preprocessing\n",
        "Following the YouTube tutorial by CodeBasics\n",
        "\n",
        "This notebook covers:\n",
        "1. Import libraries\n",
        "2. Download and load PlantVillage dataset\n",
        "3. Load images into TensorFlow dataset\n",
        "4. Visualize images\n",
        "5. Train/validation/test split (80/10/10)\n",
        "6. Apply cache, shuffle, prefetch optimizations\n",
        "7. Create preprocessing layers (resize, rescale)\n",
        "8. Create data augmentation layers\n",
        "\"\"\"\n",
        "\n",
        "# Step 1: Import necessary libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kAK8dNCoKg92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Download and load PlantVillage dataset\n",
        "import os\n",
        "\n",
        "# Define constants\n",
        "IMAGE_SIZE = 256\n",
        "BATCH_SIZE = 32\n",
        "CHANNELS = 3\n",
        "DATASET_DIR = \"/content/PlantVillage\"\n",
        "\n",
        "# Download dataset using gdown (Google Drive link)\n",
        "print(\"Downloading PlantVillage dataset from Google Drive...\")\n",
        "!gdown --folder https://drive.google.com/drive/folders/1-B_VLj1BxNfqNp0oNOQgGlVOlPEQJvz7 -O /content/PlantVillage --quiet --no-check-certificate\n",
        "\n",
        "print(\"\\nDataset download attempt complete.\")\n",
        "# Check if the directory exists\n",
        "if os.path.exists(DATASET_DIR):\n",
        "    print(f\"Dataset location: {DATASET_DIR}\")\n",
        "    print(\"Dataset directory created successfully.\")\n",
        "else:\n",
        "    print(f\"Dataset directory not found at {DATASET_DIR}.\")\n",
        "    print(\"Please check the download link and try again.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ze7bwSQJGI_1",
        "outputId": "6fcc2a9a-4f8f-4629-84c5-eb7290532c0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading PlantVillage dataset from Google Drive...\n",
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'drive.google.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "Failed to retrieve folder contents\n",
            "\n",
            "Dataset download attempt complete.\n",
            "Dataset directory not found at /content/PlantVillage.\n",
            "Please check the download link and try again.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Alternative download method using wget and a working dataset URL\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "print(\"Downloading PlantVillage dataset using wget...\")\n",
        "# Using a reliable GitHub repository with the PlantVillage dataset\n",
        "!wget -q https://github.com/spMohanty/PlantVillage-Dataset/archive/master.zip -O /content/plantvillage.zip\n",
        "\n",
        "print(\"Extracting dataset...\")\n",
        "with zipfile.ZipFile('/content/plantvillage.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/')\n",
        "\n",
        "print(\"Dataset extracted!\")\n",
        "\n",
        "# Find the potato disease folder\n",
        "import os\n",
        "for root, dirs, files in os.walk('/content/'):\n",
        "    if 'Potato___Early_blight' in dirs or any('potato' in d.lower() for d in dirs):\n",
        "        print(f\"Found dataset at: {root}\")\n",
        "        # Update the DATASET_DIR if we find the correct location\n",
        "        if 'Potato___Early_blight' in dirs:\n",
        "            DATASET_DIR = root\n",
        "            break\n",
        "\n",
        "print(f\"\\nFinal dataset directory: {DATASET_DIR}\")\n",
        "print(\"\\nChecking directory contents...\")\n",
        "if os.path.exists(DATASET_DIR):\n",
        "    contents = os.listdir(DATASET_DIR)\n",
        "    print(f\"Contents: {contents}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARppnmCqGmhb",
        "outputId": "dbef3c8a-888e-4a42-a9c7-fa15d3a7be38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading PlantVillage dataset using wget...\n",
            "Extracting dataset...\n",
            "Dataset extracted!\n",
            "Found dataset at: /content/\n",
            "Found dataset at: /content/potato_dataset\n",
            "\n",
            "Final dataset directory: /content/potato_dataset\n",
            "\n",
            "Checking directory contents...\n",
            "Contents: ['Potato___Late_blight', 'Potato___healthy', 'Potato___Early_blight']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Filter and organize potato disease dataset\n",
        "import shutil\n",
        "\n",
        "# Source directory with all plant diseases\n",
        "source_dir = \"/content/PlantVillage-Dataset-master/raw/color\"\n",
        "\n",
        "# Create a new directory for only potato diseases\n",
        "potato_dataset_dir = \"/content/potato_dataset\"\n",
        "os.makedirs(potato_dataset_dir, exist_ok=True)\n",
        "\n",
        "# Copy only potato-related folders\n",
        "print(\"Filtering potato disease images...\")\n",
        "for folder in os.listdir(source_dir):\n",
        "    if folder.startswith('Potato'):\n",
        "        src_path = os.path.join(source_dir, folder)\n",
        "        dst_path = os.path.join(potato_dataset_dir, folder)\n",
        "        if os.path.isdir(src_path):\n",
        "            shutil.copytree(src_path, dst_path)\n",
        "            print(f\"Copied {folder}: {len(os.listdir(dst_path))} images\")\n",
        "\n",
        "# Update DATASET_DIR to point to potato-only dataset\n",
        "DATASET_DIR = potato_dataset_dir\n",
        "\n",
        "print(f\"\\nPotato dataset ready at: {DATASET_DIR}\")\n",
        "print(f\"\\nClasses found:\")\n",
        "for folder in sorted(os.listdir(DATASET_DIR)):\n",
        "    folder_path = os.path.join(DATASET_DIR, folder)\n",
        "    if os.path.isdir(folder_path):\n",
        "        print(f\"  - {folder}: {len(os.listdir(folder_path))} images\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "eFSiAkxFHjPq",
        "outputId": "97334c15-6df9-4325-9486-688510a07b56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtering potato disease images...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileExistsError",
          "evalue": "[Errno 17] File exists: '/content/potato_dataset/Potato___Late_blight'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2010437526.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mdst_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpotato_dataset_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopytree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Copied {folder}: {len(os.listdir(dst_path))} images\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36mcopytree\u001b[0;34m(src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mitr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0mentries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m     return _copytree(entries=entries, src=src, dst=dst, symlinks=symlinks,\n\u001b[0m\u001b[1;32m    601\u001b[0m                      \u001b[0mignore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m                      \u001b[0mignore_dangling_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_dangling_symlinks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36m_copytree\u001b[0;34m(entries, src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0mignored_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdirs_exist_ok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m     \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m     \u001b[0muse_srcentry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mcopy2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcopy_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
            "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '/content/potato_dataset/Potato___Late_blight'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Load images into TensorFlow dataset\n",
        "print(\"Loading dataset into TensorFlow...\")\n",
        "\n",
        "# Load the dataset\n",
        "dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    DATASET_DIR,\n",
        "    seed=123,  # For reproducibility\n",
        "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "# Get class names\n",
        "class_names = dataset.class_names\n",
        "print(f\"\\nClass names: {class_names}\")\n",
        "print(f\"Number of classes: {len(class_names)}\")"
      ],
      "metadata": {
        "id": "AOLkcv7iIIhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Visualize sample images\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 12))\n",
        "for images, labels in dataset.take(1):\n",
        "    for i in range(9):\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "        plt.title(class_names[labels[i]])\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qwRXPO4aIPg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Split dataset into train, validation, and test (80/10/10)\n",
        "print(\"Splitting dataset into train, validation, and test sets...\")\n",
        "\n",
        "# Get the total number of batches\n",
        "total_batches = len(dataset)\n",
        "print(f\"Total batches: {total_batches}\")\n",
        "\n",
        "# Calculate split sizes (80% train, 10% validation, 10% test)\n",
        "train_size = int(0.8 * total_batches)\n",
        "val_size = int(0.1 * total_batches)\n",
        "test_size = total_batches - train_size - val_size\n",
        "\n",
        "print(f\"Train batches: {train_size}\")\n",
        "print(f\"Validation batches: {val_size}\")\n",
        "print(f\"Test batches: {test_size}\")\n",
        "\n",
        "# Split the dataset\n",
        "train_ds = dataset.take(train_size)\n",
        "remaining = dataset.skip(train_size)\n",
        "val_ds = remaining.take(val_size)\n",
        "test_ds = remaining.skip(val_size)\n",
        "\n",
        "print(\"\\nDataset split complete!\")"
      ],
      "metadata": {
        "id": "wGf3UG-nIWjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Apply cache, shuffle, and prefetch optimizations\n",
        "print(\"Applying performance optimizations...\")\n",
        "\n",
        "# AUTOTUNE allows TensorFlow to automatically determine optimal buffer sizes\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "# Apply optimizations to training dataset\n",
        "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "# Apply cache and prefetch to validation and test datasets\n",
        "# Note: No shuffle for val and test sets to maintain consistency\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "print(\"Optimizations applied!\")\n",
        "print(\"\\nExplanation:\")\n",
        "print(\"- cache(): Keeps images in memory after first epoch (faster training)\")\n",
        "print(\"- shuffle(): Randomizes order of training data (better generalization)\")\n",
        "print(\"- prefetch(): Prepares next batch while GPU trains current batch (reduces idle time)\")"
      ],
      "metadata": {
        "id": "Iv7ttPdMIieG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Create preprocessing layers (resize and rescale)\n",
        "print(\"Creating preprocessing layers...\")\n",
        "\n",
        "# Resize layer - ensures all images are same size (256x256)\n",
        "resize_and_rescale = tf.keras.Sequential([\n",
        "    layers.Resizing(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    layers.Rescaling(1./255)  # Normalize pixel values from [0,255] to [0,1]\n",
        "])\n",
        "\n",
        "print(\"Preprocessing layers created!\")\n",
        "print(f\"\\nResize: All images will be resized to {IMAGE_SIZE}x{IMAGE_SIZE}\")\n",
        "print(\"Rescale: Pixel values normalized from [0,255] to [0,1]\")"
      ],
      "metadata": {
        "id": "GUvM5eo9IqLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Create data augmentation layers\n",
        "print(\"Creating data augmentation layers...\")\n",
        "\n",
        "# Data augmentation helps prevent overfitting by creating variations of training images\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "    layers.RandomRotation(0.2),  # Rotate images by up to 20% (0.2 * 2œÄ radians)\n",
        "])\n",
        "\n",
        "print(\"Data augmentation layers created!\")\n",
        "print(\"\\nAugmentation techniques:\")\n",
        "print(\"- RandomFlip: Randomly flips images horizontally and vertically\")\n",
        "print(\"- RandomRotation: Randomly rotates images by up to 20%\")\n",
        "print(\"\\nThese augmentations help the model generalize better to new images!\")"
      ],
      "metadata": {
        "id": "Ndq0AFXdIxKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 11: Visualize data augmentation effects\n",
        "print(\"Visualizing data augmentation...\")\n",
        "\n",
        "plt.figure(figsize=(12, 12))\n",
        "for images, labels in train_ds.take(1):\n",
        "    first_image = images[0]\n",
        "    for i in range(9):\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        augmented_image = data_augmentation(tf.expand_dims(first_image, 0))\n",
        "        plt.imshow(augmented_image[0].numpy().astype(\"uint8\"))\n",
        "        plt.title(f\"Augmented {i+1}\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "plt.suptitle(\"Same Image with Different Augmentations\", fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nNotice how the same image appears different each time due to random augmentations!\")"
      ],
      "metadata": {
        "id": "JzOcEJyKI3og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary: Data Collection and Preprocessing Complete!\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"POTATO DISEASE CLASSIFICATION - DATA PREPROCESSING SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n‚úì Dataset Information:\")\n",
        "print(f\"  - Total images: 2152\")\n",
        "print(f\"  - Classes: {len(class_names)}\")\n",
        "print(f\"  - Class names: {class_names}\")\n",
        "print(f\"  - Image size: {IMAGE_SIZE}x{IMAGE_SIZE}x{CHANNELS}\")\n",
        "print(f\"  - Batch size: {BATCH_SIZE}\")\n",
        "\n",
        "print(\"\\n‚úì Data Split (80/10/10):\")\n",
        "print(f\"  - Training batches: 54 (approximately 1728 images)\")\n",
        "print(f\"  - Validation batches: 6 (approximately 192 images)\")\n",
        "print(f\"  - Test batches: 8 (approximately 256 images)\")\n",
        "\n",
        "print(\"\\n‚úì Applied Optimizations:\")\n",
        "print(\"  - cache(): Caches images in memory for faster access\")\n",
        "print(\"  - shuffle(): Randomizes training data order\")\n",
        "print(\"  - prefetch(): Prepares next batch while training current batch\")\n",
        "\n",
        "print(\"\\n‚úì Preprocessing Layers:\")\n",
        "print(\"  - Resizing: All images resized to 256x256\")\n",
        "print(\"  - Rescaling: Pixel values normalized from [0,255] to [0,1]\")\n",
        "\n",
        "print(\"\\n‚úì Data Augmentation Layers:\")\n",
        "print(\"  - RandomFlip: Horizontal and vertical flips\")\n",
        "print(\"  - RandomRotation: Up to 20% rotation\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"NEXT STEPS: Ready for model building and training!\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nüìù The preprocessing pipeline is ready to be used in model training.\")\n",
        "print(\"   These layers can be incorporated directly into the model architecture.\")"
      ],
      "metadata": {
        "id": "vpd_47FqI_GM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# üß† Model Building and Training\n",
        "\n",
        "Now we'll build a Convolutional Neural Network (CNN) to classify potato diseases."
      ],
      "metadata": {
        "id": "KjvF0OQKJLYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the CNN model\n",
        "from tensorflow.keras import models, layers\n",
        "\n",
        "model = models.Sequential([\n",
        "    resize_and_rescale,\n",
        "    data_augmentation,\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, CHANNELS)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(len(class_names), activation='softmax')\n",
        "])\n",
        "\n",
        "model.build(input_shape=(None, IMAGE_SIZE, IMAGE_SIZE, CHANNELS))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "qtr2t3nHMysC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"Model compiled successfully!\")"
      ],
      "metadata": {
        "id": "ZSQJORPhND0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "EPOCHS = 50\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "L-lwicSPNHdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on test dataset\n",
        "test_loss, test_accuracy = model.evaluate(test_ds)\n",
        "print(f\"\\nTest Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")"
      ],
      "metadata": {
        "id": "a37tSqRTNK50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training history\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get history data\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(EPOCHS)\n",
        "\n",
        "# Create subplots\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Plot accuracy\n",
        "ax1.plot(epochs_range, acc, label='Training Accuracy', linewidth=2)\n",
        "ax1.plot(epochs_range, val_acc, label='Validation Accuracy', linewidth=2)\n",
        "ax1.set_xlabel('Epochs', fontsize=12)\n",
        "ax1.set_ylabel('Accuracy', fontsize=12)\n",
        "ax1.set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
        "ax1.legend(fontsize=10)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot loss\n",
        "ax2.plot(epochs_range, loss, label='Training Loss', linewidth=2)\n",
        "ax2.plot(epochs_range, val_loss, label='Validation Loss', linewidth=2)\n",
        "ax2.set_xlabel('Epochs', fontsize=12)\n",
        "ax2.set_ylabel('Loss', fontsize=12)\n",
        "ax2.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
        "ax2.legend(fontsize=10)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GOTwB85x6672"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on sample images\n",
        "import numpy as np\n",
        "\n",
        "# Function to predict on a single image\n",
        "def predict_image(model, img):\n",
        "    img_array = tf.expand_dims(img, 0)  # Create batch\n",
        "    predictions = model.predict(img_array)\n",
        "    predicted_class = class_names[np.argmax(predictions[0])]\n",
        "    confidence = round(100 * np.max(predictions[0]), 2)\n",
        "    return predicted_class, confidence\n",
        "\n",
        "# Get a batch from test dataset\n",
        "for images, labels in test_ds.take(1):\n",
        "    # Display 9 sample predictions\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    for i in range(9):\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "\n",
        "        predicted_class, confidence = predict_image(model, images[i].numpy())\n",
        "        actual_class = class_names[labels[i]]\n",
        "\n",
        "        color = 'green' if predicted_class == actual_class else 'red'\n",
        "        plt.title(f\"Actual: {actual_class}\\nPredicted: {predicted_class}\\nConfidence: {confidence}%\",\n",
        "                  color=color, fontsize=10, fontweight='bold')\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "pK-NORI27J35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yuYc-Of8dphx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model with versioning (corrected)\n",
        "import os\n",
        "\n",
        "# Create models directory if it doesn't exist\n",
        "model_dir = 'saved_models'\n",
        "if not os.path.exists(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "\n",
        "# Find the next version number\n",
        "existing_versions = []\n",
        "for f in os.listdir(model_dir):\n",
        "    if f.startswith('potato_model_v') and f.endswith('.keras'):\n",
        "        try:\n",
        "            v = int(f.split('_v')[1].split('.')[0])\n",
        "            existing_versions.append(v)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "version = max(existing_versions) + 1 if existing_versions else 1\n",
        "\n",
        "# Save the model with .keras extension\n",
        "model_path = os.path.join(model_dir, f'potato_model_v{version}.keras')\n",
        "model.save(model_path)\n",
        "\n",
        "print(f\"\\n‚úÖ Model saved successfully!\")\n",
        "print(f\"üìÅ Location: {model_path}\")\n",
        "print(f\"üìä Version: {version}\")\n",
        "print(f\"üéØ Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"\\nModel can be loaded using: tf.keras.models.load_model('{model_path}')\")"
      ],
      "metadata": {
        "id": "Rdlfglk3dp-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## üéâ Project Complete!\n",
        "\n",
        "### Summary\n",
        "\n",
        "We have successfully built and trained a Convolutional Neural Network (CNN) for **Potato Disease Classification**.\n",
        "\n",
        "#### üìä Model Performance:\n",
        "- **Training Accuracy**: 92.51%\n",
        "- **Validation Accuracy**: ~96%\n",
        "- **Test Accuracy**: 89.66%\n",
        "\n",
        "#### üèóÔ∏è Model Architecture:\n",
        "- **Input Layer**: Resizing (256√ó256) + Rescaling (0-1)\n",
        "- **Data Augmentation**: Random Flip + Random Rotation\n",
        "- **3 Convolutional Blocks**:\n",
        "  - Conv2D (32 filters) ‚Üí MaxPooling\n",
        "  - Conv2D (64 filters) ‚Üí MaxPooling\n",
        "  - Conv2D (64 filters) ‚Üí MaxPooling\n",
        "- **Dense Layers**: 64 neurons (ReLU) + 3 neurons (Softmax)\n",
        "- **Total Parameters**: 3.7M\n",
        "\n",
        "#### üéØ Classes:\n",
        "1. Potato Early Blight\n",
        "2. Potato Late Blight\n",
        "3. Potato Healthy\n",
        "\n",
        "#### üíæ Model Saved:\n",
        "- Location: `saved_models/potato_model_v1.keras`\n",
        "- Can be loaded and used for predictions on new potato leaf images\n",
        "\n",
        "#### ‚úÖ Completed Steps:\n",
        "1. ‚úì Data Collection & Preprocessing\n",
        "2. ‚úì Data Visualization & Exploration\n",
        "3. ‚úì Train/Val/Test Split (80/10/10)\n",
        "4. ‚úì Data Augmentation\n",
        "5. ‚úì Model Building (CNN)\n",
        "6. ‚úì Model Training (50 epochs)\n",
        "7. ‚úì Model Evaluation\n",
        "8. ‚úì Training History Visualization\n",
        "9. ‚úì Predictions on Test Images\n",
        "10. ‚úì Model Saving with Versioning"
      ],
      "metadata": {
        "id": "h2CLHSXe7sLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Upload image\n",
        "from google.colab import files\n",
        "from PIL import Image\n",
        "import io\n",
        "import numpy as np\n",
        "\n",
        "uploaded = files.upload()\n",
        "img_path = next(iter(uploaded))\n",
        "img = Image.open(io.BytesIO(uploaded[img_path])).convert('RGB')\n",
        "img = img.resize((256, 256))   # Resize to model input\n",
        "\n",
        "# 2. Prepare image\n",
        "img_array = np.array(img) / 255.0\n",
        "img_array = np.expand_dims(img_array, 0)  # Add batch dimension\n",
        "\n",
        "# 3. Predict\n",
        "pred = model.predict(img_array)\n",
        "predicted_class = class_names[np.argmax(pred)]\n",
        "confidence = round(100 * np.max(pred), 2)\n",
        "\n",
        "print(f\"Predicted: {predicted_class}\")\n",
        "print(f\"Confidence: {confidence}%\")\n"
      ],
      "metadata": {
        "id": "WBp1Pjqycpv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('saved_models/potato_model_v2.keras')\n"
      ],
      "metadata": {
        "id": "qx4Yo7Mbx9QQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}